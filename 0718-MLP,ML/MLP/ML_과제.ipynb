{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#과제\n",
        "\n",
        "1. 하단 셀에서 lightGBM 구현하기 (상단 Ensemble 셀 모두 실행 후 진행)\n",
        "2. 그리드 서치, 랜덤 서치 각각 어떤 상황에서 효율적인지 설명\n",
        "3. 다른 데이터셋으로 Decision Tree regression task 진행해보기\n",
        "  - 원하는 데이터셋을 사용해도 괜찮고, 올려둔 bike_data 사용해도 괜찮음\n",
        "  - 직접 구현해보는 것이 어렵다면 'bike sharing data decision tree regressor' 키워드로 검색 후 참고\n"
      ],
      "metadata": {
        "id": "qovxr1A01O9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제1"
      ],
      "metadata": {
        "id": "2KXIJ2Y62Q_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
        "\n",
        "data=pd.read_csv('/content/drive/MyDrive/day.csv')\n",
        "data[\"dteday\"] = pd.to_datetime(data[\"dteday\"])\n",
        "data[\"year\"] = data[\"dteday\"].dt.year\n",
        "data[\"month\"] = data[\"dteday\"].dt.month\n",
        "data[\"day\"] = data[\"dteday\"].dt.day\n",
        "data.drop([\"instant\", \"dteday\"], axis=1, inplace=True)\n",
        "\n",
        "y = data[\"cnt\"]\n",
        "data.drop([\"cnt\"], axis=1, inplace=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 7,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'verbosity': -1,\n",
        "}\n",
        "\n",
        "k = 5\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(data, y):\n",
        "    d_train = lgb.Dataset(data.iloc[train_idx], label=y.iloc[train_idx])\n",
        "    d_valid = lgb.Dataset(data.iloc[test_idx], label=y.iloc[test_idx], reference=d_train)\n",
        "\n",
        "    model = lgb.train(params, d_train, verbose_eval=100, valid_sets=[d_valid], num_boost_round=2000, early_stopping_rounds=10)\n",
        "    y_train_pred = model.predict(data.iloc[train_idx], num_iteration=model.best_iteration)\n",
        "    y_test_pred = model.predict(data.iloc[test_idx], num_iteration=model.best_iteration)\n",
        "\n",
        "    train_rmse = np.sqrt(mean_squared_error(y.iloc[train_idx], y_train_pred))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y.iloc[test_idx], y_test_pred))\n",
        "\n",
        "    train_scores.append(train_rmse)\n",
        "    test_scores.append(test_rmse)\n",
        "\n",
        "print(\"Train Score: \", np.mean(train_scores))\n",
        "print(\"Test Score: \", np.mean(test_scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmbWnWMp6CPy",
        "outputId": "0ba16375-1c68-4d5d-ce52-84ff74e7a3dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "[100]\tvalid_0's rmse: 127.156\n",
            "[200]\tvalid_0's rmse: 116.47\n",
            "[300]\tvalid_0's rmse: 113.604\n",
            "Early stopping, best iteration is:\n",
            "[299]\tvalid_0's rmse: 113.564\n",
            "Training until validation scores don't improve for 10 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 170.892\n",
            "[200]\tvalid_0's rmse: 160.102\n",
            "Early stopping, best iteration is:\n",
            "[266]\tvalid_0's rmse: 158.769\n",
            "Training until validation scores don't improve for 10 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\tvalid_0's rmse: 151.496\n",
            "[200]\tvalid_0's rmse: 140.482\n",
            "Early stopping, best iteration is:\n",
            "[213]\tvalid_0's rmse: 140.428\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "[100]\tvalid_0's rmse: 140.038\n",
            "[200]\tvalid_0's rmse: 129.152\n",
            "Early stopping, best iteration is:\n",
            "[239]\tvalid_0's rmse: 128.41\n",
            "Training until validation scores don't improve for 10 rounds\n",
            "[100]\tvalid_0's rmse: 152.964\n",
            "Early stopping, best iteration is:\n",
            "[151]\tvalid_0's rmse: 145.345\n",
            "Train Score:  58.08034262450203\n",
            "Test Score:  137.3030299365824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##과제2\n",
        "답:"
      ],
      "metadata": {
        "id": "O5oG3_VN2L1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "그리드 서치와 랜덤 서치의 개념은 개인적으로 Batch Gradient Descent와\n",
        "Stochastic Gradient Descent의 관계를 떠올리게 되었습니다.\n",
        "하이퍼 파라미터들의 모든 조합을 시험하는 그리드 서치는 Batch Gradient Descent와 마찬가지로\n",
        "최적의 결과를 확실히 찾는 데에는 랜덤 서치보다 유리할 것입니다. 하지만 시간 등이 오래 소모되므로,\n",
        "하이퍼 파라미터 수가 적은 경우나 가능 값들의 수가 적은 경우가 더 적합한 상황이라고 생각합니다.\n",
        "반대로 랜덤 서치는 랜덤으로 선택한 조합만을 보기에, 최적으로 하는 데에는 부족하지만 시간 등의 측면에서는\n",
        "더 효율적입니다. 따라서 하이퍼 파라미터가 많은 경우나 가능 값들의 수가 커서 그리드 서치를 시행하기\n",
        "어려운 경우에 하는 것이 적합한 것 같습니다.\n",
        "올려주신 bike data 같은 경우는 비교적 하이퍼 파라미터가 적기에 grid search가 더 좋은 선택이라고 생각합니다.\n",
        "'''"
      ],
      "metadata": {
        "id": "ZTdobrMO7ACh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제3"
      ],
      "metadata": {
        "id": "OdvvbDtY2mst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "\n",
        "\n",
        "def positive_rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "positive_rmse_score = make_scorer(positive_rmse, greater_is_better=False)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "hyperparameter_grid = {\n",
        "    'max_depth': [5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(regressor, hyperparameter_grid,  n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_parameters = grid_search.best_params_\n",
        "print(\"Best Parameters: \", best_parameters)\n",
        "\n",
        "\n",
        "y_train_pred = grid_search.predict(X_train)\n",
        "train_rmse = positive_rmse(y_train, y_train_pred)\n",
        "print(\"Train RMSE: \", train_rmse)\n",
        "y_test_pred = grid_search.predict(X_test)\n",
        "test_rmse = positive_rmse(y_test, y_test_pred)\n",
        "print(\"Test RMSE: \", test_rmse)\n"
      ],
      "metadata": {
        "id": "ITQVdaeN9Ywy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0a9681-6f5a-4e5f-f285-89af0b01a8c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters:  {'max_depth': 15, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
            "Train RMSE:  55.45408493997246\n",
            "Test RMSE:  202.50138806761836\n"
          ]
        }
      ]
    }
  ]
}